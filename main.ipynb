{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import sklearn\n",
    "from sklearn import linear_model, metrics, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import r2_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collect():\n",
    "    s = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=s)\n",
    "    driver.maximize_window()\n",
    "    #driver.get('https://www.google.com')\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    # , options=chrome_options\n",
    "    \n",
    "    # driver = webdriver.Chrome(\"C:/Users/yuval/Desktop/Shahar/Data-science-Weather/chromedriver/chromedriver\", options=chrome_options)\n",
    "    # C:/Users/yuval/Desktop/Shahar/Data-science-Weather/chromedriver/chromedriver --> Shahar\n",
    "    # /Users/gal.bachar/git_wa/HIT/Data-science-Weather/chromedriver/chromedriver  --> Gal\n",
    "    df = pd.DataFrame([], columns=[\"year\", \"month\", \"day\", \"temp\",\"temp min\",\"temp max\",\"dew point\", \"humidity\", \"windspeed\", \"windspeed min\", \"windspeed max\", \"precipitation\"])\n",
    "    df_row_count=1\n",
    "\n",
    "    # switch temp from fahrenheit to celsius\n",
    "    driver.get(f\"https://www.wunderground.com/history/monthly/us/ny/new-york-city/KLGA/date/1950-1\")\n",
    "    time.sleep(2)\n",
    "    settings = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"wuSettings\")))\n",
    "    settings.click()\n",
    "    time.sleep(2)\n",
    "    celsius = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"wuSettings-quick\"]/div/a[2]')))\n",
    "    celsius.click()\n",
    "\n",
    "    for year in range(1950,2023):\n",
    "        for month in range(1,13):\n",
    "            try:\n",
    "                driver.get(f\"https://www.wunderground.com/history/monthly/us/ny/new-york-city/KLGA/date/{year}-{month}\")\n",
    "                print(f'{month}-{year}')\n",
    "            except Exception as e:\n",
    "                print(f'{e} - unable to get: {month}.{year}')\n",
    "            try:\n",
    "                table_id= WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"days\")))\n",
    "                data = table_id.find_elements(By.TAG_NAME, \"table\") # get all of the rows in the table\n",
    "                num_of_days = len(data[1].find_elements(By.TAG_NAME, \"tr\"))\n",
    "\n",
    "                for i in range(1,num_of_days):\n",
    "                    day = int(data[0].find_elements(By.TAG_NAME, \"tr\")[i].text)\n",
    "                    temp = float(data[1].find_elements(By.TAG_NAME, \"tr\")[i].text.split(' ')[1])\n",
    "                    temp_min = float(data[1].find_elements(By.TAG_NAME, \"tr\")[i].text.split(' ')[2])\n",
    "                    temp_max = float(data[1].find_elements(By.TAG_NAME, \"tr\")[i].text.split(' ')[0])\n",
    "                    dew_point = float(data[2].find_elements(By.TAG_NAME, \"tr\")[i].text.split(' ')[1])\n",
    "                    humidity = float(data[3].find_elements(By.TAG_NAME, \"tr\")[i].text.split(' ')[1])\n",
    "                    windspeed = float(data[4].find_elements(By.TAG_NAME, \"tr\")[i].text.split(' ')[1])\n",
    "                    windspeed_min = float(data[4].find_elements(By.TAG_NAME, \"tr\")[i].text.split(' ')[2])\n",
    "                    windspeed_max = float(data[4].find_elements(By.TAG_NAME, \"tr\")[i].text.split(' ')[0])\n",
    "                    precipitation = float(data[6].find_elements(By.TAG_NAME, \"tr\")[i].text)\n",
    "                    day_data = [year,month,day,temp,temp_min,temp_max,dew_point,humidity,windspeed,windspeed_min,windspeed_max,precipitation]\n",
    "                    df.loc[df_row_count] = day_data\n",
    "                    df_row_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"cloud not get {year}-{month} data: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    df.to_csv('~/Documents/nyc_weather.csv',index=False) #for debugging\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data_df, good_weather_values):\n",
    "\n",
    "    # Normalize each column to a float vlaue between 0 - 1 according its \"Good_weather_values\" definition. \n",
    "    # When 0 means \"bad\" weather and 1 means \"Good\" weather \n",
    "    \n",
    "\n",
    "    norm_df = data_df.copy()\n",
    "    # Calculating only relevant values for our target column\n",
    "    for i in norm_df.columns:\n",
    "        if i not in ['year', 'month', 'day',\"temp min\",\"temp max\", \"windspeed min\", \"windspeed max\",\"dew point\"]:\n",
    "            good_min = good_weather_values[i][0]\n",
    "            good_max = good_weather_values[i][1]\n",
    "            diff_min = good_weather_values[i][0] - norm_df[i].min()\n",
    "            diff_max = norm_df[i].max() - good_weather_values[i][1]\n",
    "            norm_df.loc[(norm_df[i] < good_min), i] = 1 - ((good_min - norm_df[i]) / diff_min)\n",
    "            norm_df.loc[(norm_df[i] > good_max), i] = 1 - ((norm_df[i] - good_max) / diff_max)\n",
    "            norm_df.loc[(norm_df[i] <= good_max) & (norm_df[i] >= good_min), i] = 1\n",
    "    \n",
    "    # Creating a Target column\n",
    "    norm_df.insert(7, \"pleasant day\", 0)\n",
    "    \n",
    "    # Calulating target column for each day base on average of temp, humidity, windspeed and precipitation values \n",
    "    norm_df.loc[(norm_df[\"temp\"]!=0),\"pleasant day\"] = (norm_df[\"temp\"] + norm_df[\"humidity\"] + norm_df[\"windspeed\"] + norm_df[\"precipitation\"]) / 4\n",
    "\n",
    "    # In case there is a NAN value in one of the relevant columns the average is done base on the other 3 columns\n",
    "    norm_df.loc[(norm_df[\"precipitation\"].isna()), \"pleasant day\"] = (norm_df[\"temp\"] + norm_df[\"humidity\"] + norm_df[\"windspeed\"]) / 3\n",
    "    norm_df.loc[(norm_df[\"temp\"].isna()), \"pleasant day\"] = (norm_df[\"precipitation\"] + norm_df[\"humidity\"] + norm_df[\"windspeed\"]) / 3\n",
    "    norm_df.loc[(norm_df[\"humidity\"].isna()), \"pleasant day\"] = (norm_df[\"temp\"] + norm_df[\"precipitation\"] + norm_df[\"windspeed\"]) / 3\n",
    "    norm_df.loc[(norm_df[\"windspeed\"].isna()), \"pleasant day\"] = (norm_df[\"temp\"] + norm_df[\"humidity\"] + norm_df[\"precipitation\"]) / 3\n",
    "    \n",
    "    # norm_df.drop(columns=[\"precipitation\"],inplace=True)\n",
    "    # norm_df.dropna(inplace=True)\n",
    "    \n",
    "    return norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_data(data):\n",
    "    data.loc[data[\"year\"] <= 2014, \"precipitation\"] = np.NaN\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_good_days_per_month(data):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "    months=['Jan', 'Feb','Mar','Apr','May','jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    res = {}\n",
    "    for i in range(0,12):\n",
    "        res[months[i]] = len(data.loc[(data['month'] == (i+1)) & (data['pleasant day'] >= 0.75)])\n",
    "    ser = pd.Series(res)\n",
    "    # print(ser)\n",
    "    ser.plot(kind='pie', ax=axes[0], title = \"All time good days per month\")\n",
    "    ser.plot(kind='line', ax=axes[1], title = \"Pleasent days per month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_good_days_trends(data):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "    months=['Jan', 'Feb','Mar','Apr','May','jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    res = {}\n",
    "    for j in range(1950,2022):\n",
    "        for i in range(0,12):\n",
    "            #print(data.loc[((data['month'] == (i+1)) & (data['year'] == j)),'pleasant day'].mean())\n",
    "            res[f\"{i + 1}-{j}\"] = round(data.loc[((data['month'] == (i+1)) & (data['year'] == j)),'pleasant day'].mean(),2)\n",
    "    ser = pd.Series(res)\n",
    "    ser.plot(kind='line', ax=axes[0], title = \"Mean good days per month\")\n",
    "\n",
    "    ser2 = pd.Series(data[\"pleasant day\"])\n",
    "    ser2.plot(kind='line', ax=axes[1], title = \"Pleasant days values over the time\")\n",
    "    ser2.loc[ser2>=0.75].plot(kind='line', ax=axes[1],linestyle=\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highly_correlated_cols(data):\n",
    "    col_correlated = set()\n",
    "    tuple_array = []\n",
    "    correlations = []\n",
    "    for i in range (len(data.corr().columns)):\n",
    "        for j in range(i):\n",
    "            if(data.corr().iloc[j, i] >= 0.5) and (data.corr().columns[i] not in col_correlated):\n",
    "                correlations.append(data.corr().iloc[i, j])\n",
    "                tuple_array.append([j, i])\n",
    "    return correlations, tuple_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_correlated_scatters(data):\n",
    "    fig, axes = plt.subplots(len(correlations), 1, figsize = (10, 12),constrained_layout=True)\n",
    "    axe_i = 0\n",
    "    index = np.argsort(correlations)\n",
    "    columns = list(data.columns)\n",
    "    for n_correlation in index:\n",
    "        col_lt, col_rt = tuple_arr[n_correlation]\n",
    "        col_left_title, col_right_title = columns[col_lt], columns[col_rt]\n",
    "        title = \"corr('%s', '%s') = %4.2f\" % (col_left_title, col_right_title, correlations[n_correlation])\n",
    "        data.plot(x = col_left_title, y = col_right_title, kind = \"scatter\",\n",
    "                ax = axes[axe_i], title = title, xlabel = col_left_title, ylabel = col_right_title)\n",
    "        axe_i = axe_i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corrupt_rows(df, num_max_missing_cols):\n",
    "    cp = df.dropna(thresh=len(df.columns)-num_max_missing_cols).copy()\n",
    "    return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutliers(df):\n",
    "    #Working on a copy DF\n",
    "    cp = df.copy()\n",
    "    all_outliers = {}\n",
    "    #Plots for each column and its outliers\n",
    "    fig, axes = plt.subplots(1, len(cp.columns) - 3, figsize = (30, 5))\n",
    "    axe_i = 0\n",
    "    #Itterating over all columns \n",
    "    for col in cp.columns:\n",
    "        # After few tests, we found out that the best Z_score_value for humidity is 2.5 \n",
    "        if col in [\"humidity\"]:\n",
    "            z_score_value=2.5\n",
    "        else:\n",
    "            #Other z_score_values should be 3 \n",
    "            z_score_value=3\n",
    "        #Ignorring Year, Month and Day columns\n",
    "        if col not in [\"year\", \"month\", \"day\"]:    \n",
    "            #Creating Histogram\n",
    "            ax = cp[col].plot(bins=50, ax=axes[axe_i], kind=\"hist\")\n",
    "            axe_i = axe_i + 1\n",
    "            ax.set(xlabel=col)\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            # looking for outliers using distances of standard deviation from the mean.\n",
    "            z_score = (cp[col] - cp[col].mean()) / cp[col].std()\n",
    "            \n",
    "            outliers = abs(z_score) > z_score_value # after some tests if we will search for lower distance we will lost important information.\n",
    "            print (f\"Number of outliers for {col} - {sum(outliers)}\")\n",
    "            all_outliers[col] = outliers\n",
    "\n",
    "    #return a dictinary with all columns and outliers\n",
    "    return all_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def putNanOutliers(df, all_outliers):\n",
    "    # Replacing outliers with NAN\n",
    "    cp = df.copy()\n",
    "    for key in all_outliers:\n",
    "        cp.loc[all_outliers[key].tolist(),key] = np.NaN\n",
    "    return cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showOutliers(all_outliers):\n",
    "    fig, axes = plt.subplots(1, len(df.columns) - 3, figsize = (30, 5))\n",
    "    axe_i = 0\n",
    "    for col in all_outliers:\n",
    "        ser = pd.Series(df.loc[all_outliers[col],col])\n",
    "        ser.plot(kind=\"hist\", ax=axes[axe_i], title = f\"Outliers for {col}\")\n",
    "        axe_i = axe_i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArray(data):\n",
    "    arr = []\n",
    "    for i in range(0,len(data)):\n",
    "        arr.append(data[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphOverTime(df,param):\n",
    "    colors=[\"r\",\"b\",\"g\",\"black\"]\n",
    "    i=0\n",
    "    for col in param:\n",
    "        c=colors[i]\n",
    "        i=i+1\n",
    "        df.plot(kind=\"line\",x=\"year\", y=[col],figsize=(15, 7), grid=True,color=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data_collect()\n",
    "df = pd.read_csv(\"~/Documents/nyc_weather.csv\")\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All data\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe describe\")\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outliers = getOutliers(df)\n",
    "fixed_data = remove_missing_data(df)\n",
    "noOut = putNanOutliers(fixed_data,all_outliers)\n",
    "\n",
    "df_no_outliers = remove_corrupt_rows(noOut, 1)\n",
    "print(\"Full DataFrame\")\n",
    "print(df.shape)\n",
    "print(\"No outliers DataFrame\")\n",
    "print(df_no_outliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show outliers\n",
    "showOutliers(all_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = remove_corrupt_rows(noOut, 1)\n",
    "print(\"Full DataFrame\")\n",
    "print(df.shape)\n",
    "print(\"No outliers DataFrame\")\n",
    "print(df_no_outliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [\"temp\",\"humidity\",\"windspeed\"]\n",
    "graphOverTime(df_no_outliers,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice weather:\n",
    "# temp 17-20\n",
    "# humidity 30-40%\n",
    "# windspeed 4-7\n",
    "# precipitation 0-1\n",
    "good_weather_values = {'temp': [17,20], 'humidity':[30,40],'windspeed':[4,7], 'precipitation':[0,1]}\n",
    "\n",
    "norm_df = normalize_data(df_no_outliers, good_weather_values)\n",
    "print(\"All data - normalized\")\n",
    "norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_good_days_per_month(norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_good_days_trends(norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = norm_df.drop(columns=['temp min','temp max','windspeed min','windspeed max'])\n",
    "\n",
    "correlations, tuple_arr = get_highly_correlated_cols(corr_df)\n",
    "high_correlated_scatters(corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_target_col(df):\n",
    "    y= df['pleasant day']\n",
    "    cp_df = df.drop('pleasant day',axis=1)\n",
    "    return cp_df,y\n",
    "\n",
    "def train_1st_model(X_train, y_train):\n",
    "    reg=LinearRegression()\n",
    "    \n",
    "    return reg.fit(X_train,y_train)\n",
    "def predict_1st(trained_1st_model, X_test):\n",
    "    predict=trained_1st_model.predict(X_test)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(norm_df.loc[(norm_df[\"month\"]==5) & (norm_df[\"day\"]==10)], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1st_train, y_1st_train = get_df_target_col(train)\n",
    "X_1st_test, y_1st_test = get_df_target_col(test)\n",
    "# print(X_1st_train.isna().sum())\n",
    "\n",
    "trained_model_1st = train_1st_model(X_1st_train, y_1st_train)\n",
    "pred_1st_vals = predict_1st(trained_model_1st, X_1st_test)\n",
    "print(pred_1st_vals.mean(),y_1st_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_1st_vals,y_1st_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_1st_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c53fc6fe09d1fc45c22adc6d00f56873dfa8a784c30473ddd47ecc0cffa49afa"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
